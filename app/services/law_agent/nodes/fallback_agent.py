from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from app.core.clients import get_llm
from app.services.law_agent.state import LawAgentState

def fallback_node(state: LawAgentState) -> LawAgentState:
    llm = get_llm()
    print("üõ°Ô∏è [FALLBACK]: K√≠ch ho·∫°t quy tr√¨nh x·ª≠ l√Ω thi·∫øu th√¥ng tin...")
    
    status = state.check_status or "NO_LAW"
    query = state.standalone_query or state.query
    docs = state.retrieved_docs or []
    has_law_context = state.has_law_context  # Use the flag from contextualize
    law_context = state.law_context  # Use the extracted law context
    
    # Content keywords  
    content_keywords = ["n·ªôi dung", "l√† g√¨", "ƒë·ªãnh nghƒ©a", "kh√°i ni·ªám", "quy ƒë·ªãnh", "quy ƒë·ªãnh g√¨", "c√≥ n·ªôi dung g√¨", "bao g·ªìm", "g·ªìm nh·ªØng g√¨"]
    is_content_question = any(keyword in query.lower() for keyword in content_keywords)
    
    # CASE 1: If law context and content question ‚Üí Answer using the law context
    if has_law_context and law_context and is_content_question:
        print("üìÑ [FALLBACK]: C√≥ ng·ªØ c·∫£nh lu·∫≠t + c√¢u h·ªèi n·ªôi dung ‚Üí S·ª≠ d·ª•ng Writer...")
        from app.services.law_agent.nodes.writer_agent import answer_node
        return answer_node(state)
    
    # TR∆Ø·ªúNG H·ª¢P 1: KH√îNG T√åM TH·∫§Y LU·∫¨T
    if status == "NO_LAW" or not docs:
        state.generation = (
            "Xin l·ªói, hi·ªán t·∫°i c∆° s·ªü d·ªØ li·ªáu c·ªßa t√¥i ch∆∞a c√≥ vƒÉn b·∫£n ph√°p l√Ω ch√≠nh x√°c v·ªÅ v·∫•n ƒë·ªÅ n√†y. "
            "ƒê·ªÉ ƒë·∫£m b·∫£o an to√†n ph√°p l√Ω, t√¥i xin ph√©p kh√¥ng t·ª± suy ƒëo√°n. B·∫°n vui l√≤ng tham v·∫•n lu·∫≠t s∆∞ tr·ª±c ti·∫øp."
        )
        state.sources = []
        state.node_trace.append("fallback")
        return state

    # TR∆Ø·ªúNG H·ª¢P 2: C√ì LU·∫¨T NH∆ØNG THI·∫æU TH√îNG TIN USER
    if status == "MISSING_INFO":
        context_text = "\n".join([f"- {d.content}" for d in docs])
        
        prompt = PromptTemplate(
            template="""B·∫°n l√† Lu·∫≠t s∆∞ t∆∞ v·∫•n.
            B·∫°n ƒë√£ t√¨m th·∫•y lu·∫≠t li√™n quan nh∆∞ng ch∆∞a th·ªÉ √°p d·ª•ng v√¨ kh√°ch h√†ng cung c·∫•p thi·∫øu th√¥ng tin.
            
            Lu·∫≠t li√™n quan:
            {context}
            
            C√¢u h·ªèi: {query}
            
            Nhi·ªám v·ª•:
            1. Kh·∫≥ng ƒë·ªãnh v·∫•n ƒë·ªÅ n√†y c√≥ quy ƒë·ªãnh ph√°p lu·∫≠t.
            2. H·ªèi l·∫°i kh√°ch h√†ng c√°c th√¥ng tin c·∫ßn thi·∫øt ƒë·ªÉ t∆∞ v·∫•n ch√≠nh x√°c h∆°n.
            
            Ph·∫£n h·ªìi:""",
            input_variables=["context", "query"]
        )
        
        chain = prompt | llm | StrOutputParser()
        clarification_msg = chain.invoke({"context": context_text, "query": query})
        
        state.generation = clarification_msg
        state.sources = list(set([d.law_name for d in docs]))
        state.node_trace.append("fallback")
        return state

    state.generation = "H·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë x√°c ƒë·ªãnh tr·∫°ng th√°i."
    state.sources = []
    state.node_trace.append("fallback")
    return state